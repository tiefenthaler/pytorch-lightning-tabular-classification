{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GENERAL THOUGHTS:**\n",
    "- How to handle SKUs with mulitple components? Does it make more sense to combine them into a single row? -> might want to try this as a next step\n",
    "\n",
    "\n",
    "**DATA PREPROCESSING:**\n",
    "\n",
    "Imbalanced data:\n",
    "- over_sampling for imbalanced data\n",
    "- cost-sensitive learning for imbalanced data\n",
    "\n",
    "categorical data:\n",
    "- Ordinal Data: The categories have an inherent order\n",
    "- Nominal Data: The categories do not have an inherent order\n",
    "\n",
    "Options:\n",
    "- nominal encode 'material_number'\n",
    "- nominal encode 'material_number_text'\n",
    "- nominal encode 'brand'\n",
    "- nominal encode 'product_area'\n",
    "- ...\n",
    "\n",
    "\n",
    "**MULTI-CLASS CLASSIFIER:**\n",
    "- Focus on \"Native Multiclass Classifiers\" as a starting point. Might try \"Binary Transformation\" or \"Hierarchical Classification\" later. https://www.projectpro.io/article/multi-class-classification-python-example/547\n",
    "- Overview models to be considered:  \n",
    "X: to be considered\n",
    "  - [ ] Naive Bayes \n",
    "  - [ ] Decision Trees\n",
    "  - [X] K-Nearest Neighbors\n",
    "  - [X] Ensemble Models (~~Random Forest~~, XGBoost)\n",
    "  - [] Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import set_config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# import xgboost as xgb\n",
    "# import lightgbm as lgbm\n",
    "\n",
    "import imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import optuna\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Iterable, List, Optional, Tuple, Union, Literal\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "# from torcheval.metrics.functional import multiclass_f1_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data using Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets, DataLoaders & LightningDataModule\n",
    "- Logic of using `Datasets`, `DataLoaders` & `LightningDataModule` for **Tabular Data**  \n",
    "    Since we are using tabular data and want to perform non tensor based processing to our date, we use Datasets, DataLoaders & LightningDataModule in a different manner as we would do when applying tensor operations (incl. tensor based preprpcessing) only.  \n",
    "    - `LightningDataModule` \n",
    "        Our LightningDataModule builds the wrapper around this process, with the following intensions:\n",
    "        - `def prepare_data`  \n",
    "        Data Operations that only should be performed once on the data (and should not be performed on a distributed manner). Prepares the data for data pre-processing e.g. using sklearn.  \n",
    "            - Loading the data from .csv to pd.DataFrame\n",
    "            - General dataset preperations like feature selection and data type correction\n",
    "        - `def setup`  \n",
    "        First, data operations (like shuffle, split data, categorical encoding, normalization, etc.), which any dataframe should undergo before feeding into the dataloader will be performed here. Since we use sklearn functionalities for our tabular pre-processing pipeline the data input and output of the pre-processing is a tabular format (dataframe) and not a tensor format.\n",
    "        Second, the outcome of `def setup` are `Dataset` objects for each split (e.g. train, val, test, pred), which is a wrapper around the pre-processed tabular data and provides the samples and their corresponding labels (ideally in tensor format) in a specific way to be compatible for the model(s).\n",
    "            - `Dataset`  \n",
    "            Dataset provides the samples and their corresponding labels in a specific way to be compatible for the model(s). We define the input for our tabular use case as a DataFrame, while the output should generally be a tensor. In our case the output is a tuple of a flattern tensor representing all features and a tensor for the target variable (label). This aligns with the input if a simple MLP model. For more complex models, e.g. that handle continous and categorical variables differently, this should be adapted.  \n",
    "            The class performs specific data type correction for to use of Neural Networks (e.g. ensure that all outputs are numeric values of the correct type depeding of they are categorical or continous nature).\n",
    "        - `def train/val/test/prediction_dataloader`  \n",
    "        Creates our dataloaders within the LightningDataModule. See usage below.\n",
    "            - `DataLoader` \n",
    "            DataLoader wraps an iterable around the Dataset to enable easy access to the samples during training and inference. The Dataset defines the format of the data that is passed to our model. The DataLoader retrieves our dataset’s features and labels. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval, which is handled by the DataLoader. Input and output is a tensor. https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "- Excorsion: Classical approach of using `Datasets`, `DataLoaders` & `LightningDataModule` for e.g. images, ...  \n",
    "The main difference is the usage of tensors instead of a dataframe for efficent GPU usage.\n",
    "    - `LightningDataModule` \n",
    "        Our LightningDataModule builds the wrapper around this process. It encapsulates training, validation, testing, and prediction dataloaders, as well as any necessary steps for data processing, downloads, and transformations. https://lightning.ai/docs/pytorch/stable/data/datamodule.html\n",
    "        - `def prepare_data`  \n",
    "        Loads the data and does general processing befor transfomring to a tensor, so efficent tensor operations can be enabled in `setup'.\n",
    "        - `def setup`  \n",
    "        Efficent tensor operations (like shuffle, split data, categorical encoding, normalization, etc.), which any dataframe should undergo before feeding into the dataloader.\n",
    "        - `def train/val/test_dataloader`  \n",
    "        Creates our dataloaders within the LightningDataModule.\n",
    "    - `Dataset`\n",
    "    Class to create tabular dataset to follow PyTorch Lightning conventions (eventhough we are working on tabular data), where the data for feature variables and the target variable are often already provided in a combined way (e.g. contrary to images and corresbonding labels). For \"classical\" approaches a Dataset class is often used at the start of the machine learning pipeline to provide the data in a format (e.g. combine images and corresponding labels, which are typically not provided in the same file) for further processing and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data: pd.DataFrame = None,\n",
    "            continuous_cols: List[str] = None,\n",
    "            categorical_cols: List[str] = None,\n",
    "            target: List[str] = None,\n",
    "            task: Literal['classification', 'regression'] = 'classification',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This class is customized for tabular related data for the use of classification and regression. Returns the tabular data as tensor format.\n",
    "        Input data must be of numeric nature and should be ordinal or label encoded. This should be covered by a related LightningDataModule.\n",
    "        Besides the standard functionality of the 'Dataset' class it provides data type correction to fit the requirements of Neural Networks and for efficent use of Neural Networks.\n",
    "        NOTE: The original intention of using a Torch Dataset Class, is to provide the output of the data as tensors for further use of pytorch\n",
    "        and to enable tensor operations. For our (and most) tabular datasets we neglect the aspect of tensor operations, since we do the data transformations (e.g. using sklearn),\n",
    "        which are not tensor based, within L.LightningDataModule. The TabularDataset class is used to provide the data as tensors to the DataLoaders as a final step after data prepressing.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): Pandas DataFrame to load during training.\n",
    "            continuous_cols (List[str], optional): A list of names of continuous columns.\n",
    "            categorical_cols (List[str], optional): A list of names of categorical columns. These columns must be ordinal or label encoded beforehand.\n",
    "            target (List[str], optional): A list of strings with target column name(s).\n",
    "            task (str, optional): Whether it is a classification or regression task. If classification, it returns a LongTensor as target.\n",
    "        Returns:\n",
    "            Corrected tabular data as tensor format.\n",
    "        \"\"\"\n",
    "        # self.data = data\n",
    "        self.task = task\n",
    "        self.n_samples = data.shape[0]\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.continuous_cols = continuous_cols\n",
    "        self.target = target\n",
    "\n",
    "        # NOTE: input data must be ordinal or label encoded\n",
    "\n",
    "        # target handling\n",
    "        if self.target:\n",
    "            self.y = data[self.target].astype(np.float32).values # for regression task\n",
    "            if self.task == \"classification\":\n",
    "                # self.y = self.y.reshape(-1, 1).astype(np.int64) # for classification task, reshape for multi class classification (must be handled accordingly in the model)\n",
    "                self.y = self.y.astype(np.int64) # for classification task\n",
    "        else:\n",
    "            self.y = np.zeros((self.n_samples, 1))  # for regression task\n",
    "            if self.task == \"classification\":\n",
    "                self.y = self.y.astype(np.int64) # for classification task\n",
    "        \n",
    "        # feature handling\n",
    "        self.categorical_cols = self.categorical_cols if self.categorical_cols else []\n",
    "        self.continuous_cols = self.continuous_cols if self.continuous_cols else []\n",
    "        if self.continuous_cols:\n",
    "            self.continuous_X = data[self.continuous_cols].astype(np.float32).values\n",
    "        if self.categorical_cols:\n",
    "            self.categorical_X = data[self.categorical_cols].astype(np.int64).values\n",
    "            self.categorical_X = self.categorical_X.astype(np.int64) # TODO: remove\n",
    "\n",
    "\n",
    "    @property\n",
    "    def get_dataframe(self):\n",
    "        \"\"\"Creates and returns the dataset as a pandas dataframe.\"\"\"\n",
    "        if self.continuous_cols or self.categorical_cols:\n",
    "            df = pd.DataFrame(\n",
    "                dict(zip(self.continuous_cols, self.continuous_X.T)) |\n",
    "                dict(zip(self.categorical_cols, self.categorical_X.T))\n",
    "           )\n",
    "        else:\n",
    "            df = pd.DataFrame()\n",
    "        df[self.target] = self.y # add target column\n",
    "\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Generates one single sample of data of the dataset (row)\n",
    "        and applies transformations to that sample if defined.\n",
    "        Called iteratively based on batches and batch_size.\n",
    "        Args:\n",
    "            idx (int): index (between ``0`` and ``len(dataset) - 1``)\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Dict[str, torch.Tensor], torch.Tensor]: x and y for model\n",
    "        \"\"\"\n",
    "        \n",
    "        return {\n",
    "            \"continuous\": (torch.as_tensor(self.continuous_X[idx]) if self.continuous_cols else torch.Tensor()),\n",
    "            \"categorical\": (torch.as_tensor(self.categorical_X[idx]) if self.categorical_cols else torch.Tensor()), #  dtype=torch.int64\n",
    "            \"target\": torch.as_tensor(self.y[idx]) # , dtype=torch.long\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Data using DataLoader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataModuleClassificationPACKAGING(L.LightningDataModule):\n",
    "    \"\"\"\n",
    "    The class processes the data accordingly, so that the output meets the requirments to be further used by PyTorch/Lightning.\n",
    "    A shareble, reusable class that encapsulates data loading and data preprocessing logic for classification.\n",
    "    The class provides general data handeling and very specific data handeling to the 'Packaging Dataset' ('number` and 'object' types as variables are supported, but no other e.g. like 'date').\n",
    "    NOTE: In addition, the original intention of using a L.LightningDataModule is to performe data operations on tensors to improve compute performance. For our (and most) tabular datasets we neglect this aspect,\n",
    "    since we perform data transformations, which are not tensor based. Therefore data preprocessing and transformations are organized within the class methods 'prepare_data' and 'setup',\n",
    "    based on if they should be performed a single time only or multiple times.\n",
    "    NOTE: Be aware of the status of your pre-processing pipeline / transformers (data they are fit on) - performance optimization vs. final evaluation vs. inference only.\n",
    "          The stage parameter ('fit' or 'inference') in def _preprocessing_pipeline controls this internal logic.\n",
    "    NOTE: Training, validation, testing and prediction are triggered by the Lightning Trainer() methods (.fit(), .validate(), .test() and .predict()).\n",
    "          The stage parameter ('fit') controles the internal logic to provide the correct data splitting and dataloader generation.\n",
    "\n",
    "    Args:\n",
    "        L (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        continuous_cols: List[str] = None,\n",
    "        categorical_cols: List[str] = None,\n",
    "        target: List[str] = None,\n",
    "        oversampling: bool = False,\n",
    "        task: Literal['classification', 'regression'] = 'classification',\n",
    "        test_size: Optional[float] = None,\n",
    "        val_size: Optional[float] = None,\n",
    "        batch_size: int = 64,\n",
    "        batch_size_inference: Optional[int] = None,\n",
    "        SEED: Optional[int] = 42\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.categorical_cols = categorical_cols if categorical_cols else []\n",
    "        self.continuous_cols = continuous_cols if continuous_cols else []\n",
    "        self.task = task\n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "        self.target = target\n",
    "        self.oversampling = oversampling\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_size_inference = self.batch_size if not batch_size_inference else batch_size_inference\n",
    "        self.stage_setup = None\n",
    "        self.SEED = SEED\n",
    "\n",
    "        self._prepare_data_called = False\n",
    "        # self._setup_called = False\n",
    "\n",
    "    def _prepare_data(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Performs general, use case independent data input handeling and data type handling.\n",
    "        Used internal in 'prepare_data' for train, val and test dataloaders and in 'inference_dataloader' for prediction.\n",
    "        Target specific handelings are performed in 'perpare_data' to avoid conflicts during inference only scenarios where the target is not available.\n",
    "        General data preparation involves:\n",
    "            - transform target variable to data type 'object' for classificatiomn tasks and to data type 'float32' for regression tasks.\n",
    "            - transform continuous feature variables to data type 'np.float32'.\n",
    "            - transform categorical feature variables to data type 'object'.\n",
    "            - update the processed dataframe accordingly and drops not specified columns.\n",
    "        \"\"\"\n",
    "        if self.task == 'classification':\n",
    "            # transform target variable to data type 'object'\n",
    "            data[self.target] = data[self.target].astype('object').values\n",
    "        elif self.task == 'regression':\n",
    "            # transform target variable to data type 'float32'\n",
    "            data[self.target] = data[self.target].astype(np.float32).values\n",
    "\n",
    "        if len(self.continuous_cols) > 0:\n",
    "            # continuous_cols will be transfomred to float32 ('32' for performance reasons) since NNs do not handle int properly.\n",
    "            data[self.continuous_cols] = data[self.continuous_cols].astype(np.float32).values\n",
    "        if len(self.categorical_cols) > 0:\n",
    "            # ensure that all categorical variables are of type 'object'\n",
    "            data[self.categorical_cols] = data[self.categorical_cols].astype('object').values\n",
    "            \n",
    "        if (len(self.continuous_cols) > 0) or (len(self.categorical_cols) > 0):\n",
    "            self.feature_cols = self.continuous_cols + self.categorical_cols\n",
    "            pass\n",
    "        else:\n",
    "            raise TypeError(\"Missing required argument: 'continuous_cols' and/or 'categorical_cols'\")\n",
    "        \n",
    "        # Define a subset based on continuous_cols and categorical_cols\n",
    "        data = data[self.continuous_cols + self.categorical_cols + self.target]\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def _preprocessing_pipeline(self, X: pd.DataFrame = None, y: pd.DataFrame = None, stage: str = 'fit') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        PREPROCESSING PIPELINE\n",
    "        Used internal in 'setup' for train, val and test dataloaders and in 'inference_dataloader',\n",
    "        as well as for inverse transformations.\n",
    "        NOTE: TabularDatasetPACKAGING prepares data for prediction only accordingly to support _preprocessing_pipeline.\n",
    "        \"\"\"\n",
    "        \n",
    "        # create pipeline for fit scenario, use existing pipeline for inference scenario\n",
    "        if stage == 'fit':\n",
    "            # numerical feature processing\n",
    "            numerical_features = X.select_dtypes(include='number').columns.tolist()\n",
    "            numeric_feature_pipeline = Pipeline(steps=[\n",
    "                ('impute', SimpleImputer(strategy='median')),\n",
    "                ('scale', StandardScaler())\n",
    "            ])\n",
    "            # categorical feature processing\n",
    "            categorical_features = X.select_dtypes(exclude='number').columns.tolist()\n",
    "            categorical_feature_pipeline = Pipeline(steps=[\n",
    "                ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "                # ('label', LabelEncoder()),\n",
    "                ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)), # ordinal is used instead of label encoder to avoid conflicts with inference or \n",
    "                #conflicts caused by data splits of categories with low numerber of classesonly scenarios\n",
    "                # ('one_hot', OneHotEncoder(handle_unknown='ignore', max_categories=None, sparse_output=False))\n",
    "            ])\n",
    "            # apply both pipeline on seperate columns using \"ColumnTransformer\"\n",
    "            self.preprocess_pipeline = ColumnTransformer(transformers=[\n",
    "                ('number', numeric_feature_pipeline, numerical_features),\n",
    "                ('category', categorical_feature_pipeline, categorical_features)],\n",
    "                verbose_feature_names_out=False)\n",
    "            self.preprocess_pipeline.set_output(transform=\"pandas\")\n",
    "\n",
    "            # ordinal is used instead of label encoder to avoid conflicts with inference or \n",
    "            # conflicts caused by data splits of categories with low numerber of classesonly scenarios\n",
    "            self.label_encoder_target = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1) \n",
    "            # self.label_encoder_target = LabelEncoder()\n",
    "        \n",
    "        if stage == 'fit':\n",
    "            X_transformed = self.preprocess_pipeline.fit_transform(X)\n",
    "            y_transformed = pd.DataFrame(data=self.label_encoder_target.fit_transform(y.values.reshape(-1, 1)), index=y.index, columns=y.columns)\n",
    "        elif stage == 'inference':\n",
    "            X_transformed = self.preprocess_pipeline.transform(X)\n",
    "            y_transformed = pd.DataFrame(data=self.label_encoder_target.transform(y.values.reshape(-1, 1)), index=y.index, columns=y.columns)\n",
    "        else:\n",
    "            raise ValueError(f\"Missing required argument 'stage', must be 'fit' or 'inference', got {stage}\")\n",
    "        \n",
    "        return pd.concat([X_transformed, y_transformed], axis=1)\n",
    "    \n",
    "\n",
    "    def prepare_data(self, shuffle: bool = False):\n",
    "        \"\"\" Custom data specific operations and basic tabular specific operations that only should be performed once on the data (and should not be performed on a distributed manner).\n",
    "        Load the data as Tabular Data as a Pandas DataFrame from a .csv file and performs custom data processing related to loading a .csv file (data type correction) and defining a subset of features.\n",
    "        In addition \"_prepare_data\" performace general data preparation for the classification/regression task and perform basic data error handeling. General data preparation involves:\n",
    "            - transform target variable to data type 'object'.\n",
    "            - update the processed dataframe accordingly and drops not specified columns.\n",
    "            - shuffle the data (rows).\n",
    "        \"\"\"\n",
    "\n",
    "        # USE CASE SPECIFIC DATA HANDLING\n",
    "        self.data = pd.read_csv(self.data_dir, sep='\\t')\n",
    "        # for inference mode, as the target might not be provided in the data,\n",
    "        # ensures pre-processing pipeline completes correctly.\n",
    "        if 'packaging_category' not in self.data.columns:\n",
    "            # Insert an empty column at the end (position=-1)\n",
    "            self.data.insert(len(self.data.columns), 'packaging_category', np.nan)\n",
    "        # select a subset\n",
    "        self.data = self.data[[\n",
    "            'material_number',\n",
    "            'brand',\n",
    "            'product_area',\n",
    "            'core_segment',\n",
    "            'component',\n",
    "            'manufactoring_location',\n",
    "            'characteristic_value',\n",
    "            'material_weight', \n",
    "            'packaging_code',\n",
    "            'packaging_category',\n",
    "        ]]\n",
    "        self.data['material_number'] = self.data['material_number'].astype('object')\n",
    "        \n",
    "        if self.oversampling:\n",
    "            # NOTE: Oversampling so each class has at least 100 sample; to properly represent minority classes during training and evaluation\n",
    "            X = self.data.iloc[:, :-1]\n",
    "            y = self.data.iloc[:, -1]  # the last column is the target, ensured based on section before (# select a subset)\n",
    "            dict_oversmapling = {\n",
    "                'Metal Cassette': 100,\n",
    "                'Carton tube with or w/o': 100,\n",
    "                'Wooden box': 100,\n",
    "                'Fabric packaging': 100,\n",
    "                'Book packaging': 100\n",
    "            }\n",
    "            oversampler = RandomOverSampler(sampling_strategy=dict_oversmapling, random_state=SEED)\n",
    "            X_oversample, y_oversample = oversampler.fit_resample(X, y)\n",
    "            self.data = pd.concat([X_oversample, y_oversample], axis=1)\n",
    "\n",
    "        # GENERAL DATA HANDLING\n",
    "        self.data = self._prepare_data(self.data)\n",
    "\n",
    "        # shuffle data\n",
    "        if shuffle is True: self.data = self.data.sample(frac=1)\n",
    "\n",
    "        self.n_samples = self.data.shape[0]\n",
    "\n",
    "        self._prepare_data_called = True\n",
    "\n",
    "    \n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Data Operations (like shuffle, split data, categorical encoding, normalization, etc.) that will be performed multiple times (on each single GPU indipendently), which any dataframe should undergo before feeding into the dataloader.\n",
    "        Since on tabular data, operations like transformations (categorical encoding, normalization, etc.) needs to be performed with respect to all samples (respectively separat per train, val, test split),\n",
    "        most operations are not performed in DDP way. See class docstring for further details regarding tabular data and tensor transformations.\n",
    "\n",
    "        Args:\n",
    "            test_size (Optional[float], optional):\n",
    "                Defines the hold out split that should be used for final performance evaluation. If 'None' no split will be performed and all data is used in 'fit'\n",
    "            val_size (Optional[float], optional):\n",
    "                Defines an additional split on the train data that should be used for model optimization. If 'None' no val split will be performed and all train data is used in 'fit'\n",
    "            stage (Optional[str], optional):\n",
    "                Internal parameter to distinguish between 'fit' and 'inference'. Defaults to None.\n",
    "\n",
    "        TODO: check interaction of 'stage' with Lightning Module inherent use of 'stage'.\n",
    "        \"\"\"\n",
    "\n",
    "        self.stage_setup = stage\n",
    "\n",
    "        if not self._prepare_data_called:\n",
    "            raise RuntimeError(\"'prepare_data' needs to be called before 'setup'\")\n",
    "\n",
    "        # Define features and target\n",
    "        X = self.data.iloc[:, :-1]\n",
    "        y = self.data.iloc[:, -1]  # the last column is the target, ensured by calling 'prepare_data' upfront\n",
    "\n",
    "\n",
    "        if stage in ('fit', 'validate', 'test'):\n",
    "            # Generate train, val and test data splits\n",
    "            if self.test_size is not None:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size=self.test_size, stratify=y, random_state=self.SEED\n",
    "                )\n",
    "                X_train = pd.DataFrame(data=X_train, columns=X.columns)\n",
    "                y_train = pd.DataFrame(data=y_train, columns=[y.name])\n",
    "                X_test = pd.DataFrame(data=X_test, columns=X.columns)\n",
    "                y_test = pd.DataFrame(data=y_test, columns=[y.name])\n",
    "                if (self.val_size is not None) and (self.test_size is not None):\n",
    "                    X_train, X_val, y_train, y_val = train_test_split(\n",
    "                        X_train, y_train, test_size=self.val_size, stratify=y_train, random_state=self.SEED\n",
    "                    )\n",
    "                    X_train = pd.DataFrame(data=X_train, columns=X.columns)\n",
    "                    y_train = pd.DataFrame(data=y_train, columns=[y.name])\n",
    "                    X_val = pd.DataFrame(data=X_val, columns=X.columns)\n",
    "                    y_val = pd.DataFrame(data=y_val, columns=[y.name])\n",
    "            else:\n",
    "                X_train = X\n",
    "                y_train = pd.DataFrame(data=y, columns=[y.name])\n",
    "        elif stage == 'predict':\n",
    "            X_pred = X\n",
    "            y_pred = pd.DataFrame(data=y, columns=[y.name])\n",
    "        else:\n",
    "            raise ValueError(f\"Stage must be 'fit', 'validate', 'test' or 'predict', got {stage}\")\n",
    "        \n",
    "\n",
    "        # pre-process data\n",
    "        if stage in ('fit', 'validate', 'test'):\n",
    "            tabular_train = self._preprocessing_pipeline(X_train, y_train, stage='fit')\n",
    "            # the logic ensures that y_train is during all training scenarios and inference scenario always the right reference for number of classes\n",
    "            self.n_classes = len(tabular_train[y.name].unique())\n",
    "            if self.test_size is not None:\n",
    "                tabular_test = self._preprocessing_pipeline(X_test, y_test, stage='inference')\n",
    "            if (self.val_size is not None) and (self.test_size is not None):\n",
    "                tabular_val = self._preprocessing_pipeline(X_val, y_val, stage='inference')\n",
    "        elif stage == 'predict':\n",
    "            tabular_predict = self._preprocessing_pipeline(X_pred, y_pred, stage='inference')\n",
    "\n",
    "        # create datasets\n",
    "        # NOTE: instanziation of datasets (train, val test) in stage == ('fit', 'validate', 'test') is controlled by self.test_size and self.val_size\n",
    "        #       instanziation of datasets (predict) is controlled by stage == 'predict'\n",
    "        if stage in ('fit', 'validate', 'test'):\n",
    "            self.train_dataset = TabularDataset(\n",
    "                    data=tabular_train,\n",
    "                    continuous_cols=self.continuous_cols,\n",
    "                    categorical_cols=self.categorical_cols,\n",
    "                    target=self.target,\n",
    "                    task='classification'\n",
    "                )\n",
    "            if self.test_size is not None:\n",
    "                self.test_dataset = TabularDataset(\n",
    "                    data=tabular_test,\n",
    "                    continuous_cols=self.continuous_cols,\n",
    "                    categorical_cols=self.categorical_cols,\n",
    "                    target=self.target,\n",
    "                    task='classification'\n",
    "                )\n",
    "            if (self.val_size is not None) and (self.test_size is not None):\n",
    "                self.val_dataset = TabularDataset(\n",
    "                    data=tabular_val,\n",
    "                    continuous_cols=self.continuous_cols,\n",
    "                    categorical_cols=self.categorical_cols,\n",
    "                    target=self.target,\n",
    "                    task='classification'\n",
    "                )\n",
    "        elif stage == 'predict':\n",
    "            self.predict_dataset = TabularDataset(\n",
    "                data=tabular_predict,\n",
    "                continuous_cols=self.continuous_cols,\n",
    "                categorical_cols=self.categorical_cols,\n",
    "                target=self.target,\n",
    "                task='classification'\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Stage must be 'fit', 'validate', 'test' or 'predict', got {stage}\")\n",
    "    \n",
    "        # self._setup_called = True\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Dataloader that the Trainer fit() method uses.\n",
    "        Args:\n",
    "            batch_size (Optional[int], optional): Batch size. Defaults to `self.batch_size`.\n",
    "        Returns:\n",
    "            DataLoader: Train dataloader\n",
    "        \"\"\"\n",
    "        # if not self._setup_called:\n",
    "        #     raise RuntimeError(\"'setup' needs to be called before 'train_dataloader'\")\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Dataloader that the Trainer fit() and validate() methods uses.\n",
    "        Args:\n",
    "            batch_size (Optional[int], optional): Batch size. Defaults to `self.batch_size`.\n",
    "        Returns:\n",
    "            DataLoader: Validation dataloader\n",
    "        \"\"\"\n",
    "        # if not self._setup_called:\n",
    "        #     raise RuntimeError(\"'setup' needs to be called before 'val_dataloader'\")\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Dataloader that the Trainer test() method uses.\n",
    "        \"\"\"\n",
    "        # if not self._setup_called:\n",
    "        #     raise RuntimeError(\"'setup' needs to be called before 'test_dataloader'\")\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Dataloader that the Trainer predict() method uses.\n",
    "        Used for predictions for data with unknow target (labes) by performing the following:\n",
    "        - creates TabularDatasetPACKAGING dataset from csv file. TabularDatasetPACKAGING prepares data for prediction only accordingly to support _preprocessing_pipeline.\n",
    "        - _prepare_data\n",
    "        - _preprocessing_pipeline\n",
    "        \"\"\"\n",
    "        if self.stage_setup == 'predict':\n",
    "            return DataLoader(self.predict_dataset, batch_size=self.batch_size_inference, shuffle=False)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = TabularDataModuleClassificationPACKAGING(\n",
    "    data_dir='../../data/output/df_ml.csv',\n",
    "    continuous_cols=['material_weight'],\n",
    "    categorical_cols=[\n",
    "        'material_number',\n",
    "        'brand',\n",
    "        'product_area',\n",
    "        'core_segment',\n",
    "        'component',\n",
    "        'manufactoring_location',\n",
    "        'characteristic_value',\n",
    "        'packaging_code'\n",
    "    ],\n",
    "    target=['packaging_category'],\n",
    "    oversampling=True,\n",
    "    test_size=0.2,\n",
    "    val_size=0.2,\n",
    "    batch_size=64,\n",
    "    SEED=SEED # Ensure same data split as in other notebooks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Run dm.prepare_data() and dm.setup() to get information from the dataset to build the model.\n",
    "dm.prepare_data()\n",
    "dm.setup(stage='fit')\n",
    "# dm.data.info()\n",
    "# dm.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataloader_output(dm: TabularDataModuleClassificationPACKAGING = None, out: Dict[str, torch.Tensor] = None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dm: pre-processed datamodule from class TabularDataModuleClassificationPACKAGING\n",
    "        out: output from the dataloader\n",
    "    \"\"\"\n",
    "    continuous_x = out['continuous']\n",
    "    categorical_x = out['categorical']\n",
    "    y = out['target']\n",
    "\n",
    "    assert isinstance(y, torch.Tensor), \"y output should be a torch.Tensor\"\n",
    "\n",
    "    # check continuous features for nans and finite\n",
    "    assert torch.isfinite(continuous_x).all(), f\"Values for {categorical_x} should be finite\"\n",
    "    assert not torch.isnan(continuous_x).any(), f\"Values for {categorical_x} should not be nan\"\n",
    "    assert continuous_x.dtype == torch.float32, f\"Values for {categorical_x} should be of type float32\"\n",
    "    # check categorical features for nans and finite\n",
    "    assert torch.isfinite(categorical_x).all(), f\"Values for {categorical_x} should be finite\"\n",
    "    assert not torch.isnan(categorical_x).any(), f\"Values for {categorical_x} should not be nan\"\n",
    "    assert categorical_x.dtype == torch.int64, f\"Values for {categorical_x} should be of type int64\"\n",
    "\n",
    "    # check target for nans and finite\n",
    "    assert torch.isfinite(y).all(), \"Values for target should be finite\"\n",
    "    assert not torch.isnan(y).any(), \"Values for target should not be nan\"\n",
    "    assert y.dtype == torch.int64, \"Values for target should be of type int64\"\n",
    "\n",
    "    # check shape\n",
    "    assert continuous_x.size(1) == dm.data[dm.continuous_cols].shape[1]\n",
    "    assert categorical_x.size(1) == dm.data[dm.categorical_cols].shape[1]\n",
    "\n",
    "check_dataloader_output(dm, next(iter(dm.train_dataloader())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataloader_output(dm: TabularDataModuleClassificationPACKAGING = None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dm: pre-processed datamodule from class TabularDataModuleClassificationPACKAGING\n",
    "    \"\"\"\n",
    "    num_epochs = 1\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for batch_idx, dict in enumerate(dm.train_dataloader()):\n",
    "            print(\"Batch:\", batch_idx)\n",
    "            if batch_idx >= 2:\n",
    "                break\n",
    "            for k, v in dict.items():\n",
    "                print(k, v.shape)\n",
    "            \n",
    "            network_input = torch.cat((dict[\"continuous\"], dict[\"categorical\"]), dim=1)\n",
    "            print(\"Shape of network input:\", network_input.shape, \"Data Types Cont:\", [column.dtype for column in dict[\"continuous\"].unbind(1)], \"Data Types Cat:\", [column.dtype for column in dict[\"categorical\"].unbind(1)])\n",
    "            # print(\"Shape of target flatten:\", dict['target'].flatten().shape, \"Data Types:\", dict['target'].flatten().dtype)\n",
    "            print(\"Shape of target flatten:\", dict['target'].shape, \"Data Types:\", dict['target'].dtype)\n",
    "            print(\"Target from current batch:\", dict['target'][:5])\n",
    "            print(\"Dataloader output from current batch, Cont:\", dict[\"continuous\"][:3])\n",
    "            print(\"Dataloader output from current batch, Cat:\", dict[\"categorical\"][:3])\n",
    "\n",
    "print_dataloader_output(dm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN without HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassTabularMLP(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        hidden_size: int,\n",
    "        n_hidden_layers: int,\n",
    "        activation_class: nn.ReLU = nn.ReLU,\n",
    "        dropout: float = None,\n",
    "        norm: bool = True,\n",
    "        # loss_function: nn.CrossEntropyLoss = nn.CrossEntropyLoss(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.activation_class = activation_class\n",
    "        self.dropout = dropout\n",
    "        self.norm = norm\n",
    "        # self.loss_function = loss_function\n",
    "\n",
    "        # #### define the MLP ####\n",
    "        # input layer\n",
    "        module_list = [nn.Linear(input_size, hidden_size), activation_class()]\n",
    "        if dropout is not None:\n",
    "            module_list.append(nn.Dropout(dropout))\n",
    "        if norm:\n",
    "            module_list.append(nn.LayerNorm(hidden_size))\n",
    "        # hidden layers\n",
    "        for _ in range(n_hidden_layers):\n",
    "            module_list.extend([nn.Linear(hidden_size, hidden_size), activation_class()])\n",
    "            if dropout is not None:\n",
    "                module_list.append(nn.Dropout(dropout))\n",
    "            if norm:\n",
    "                module_list.append(nn.LayerNorm(hidden_size))\n",
    "        # output layer\n",
    "        module_list.append(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "        self.sequential = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the MLP.\"\"\"\n",
    "        # concatenate continuous and categorical features\n",
    "        network_input = torch.cat((x[\"continuous\"], x[\"categorical\"]), dim=1) # NOTE: converts all data types to float32 (respective to the data type of the first element)\n",
    "        return self.sequential(network_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassTabularLightningModule(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes: int,\n",
    "        model: torch.nn.Module,\n",
    "        learning_rate: float = 0.001,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_acc = MulticlassF1Score(num_classes=self.n_classes, average='weighted') # torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "        self.val_acc = MulticlassF1Score(num_classes=self.n_classes, average='weighted') # torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "        self.test_acc = MulticlassF1Score(num_classes=self.n_classes, average='weighted') # torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the MLP.\"\"\"\n",
    "        return self.model(x)\n",
    "    \n",
    "    def _shared_step(self, batch: Dict[str, torch.Tensor], batch_idx):\n",
    "        x = {key: batch[key] for key in [\"continuous\", \"categorical\"]}\n",
    "        y = batch['target'].flatten() # flatten to match input shape of F.cross_entropy\n",
    "        y_hat = self.forward(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        y_hat = torch.argmax(y_hat, dim=1) # provides the class with the highest probability to match the shapee of y\n",
    "        return (loss, y_hat, y)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, y_hat, y = self._shared_step(batch, batch_idx)\n",
    "        self.log(f\"train_loss\", loss)\n",
    "        self.train_acc(y_hat, y)\n",
    "        self.log(\"train_F1_macro_weighted\", self.train_acc, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, y_hat, y = self._shared_step(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.val_acc(y_hat, y)\n",
    "        self.log(\"val_F1_macro_weighted\", self.val_acc, prog_bar=True)\n",
    "        return\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, y_hat, y = self._shared_step(batch, batch_idx)\n",
    "        self.test_acc(y_hat, y)\n",
    "        self.log(\"test_F1_macro_weighted\", self.test_acc)\n",
    "        return\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x = {key: batch[key] for key in [\"continuous\", \"categorical\"]}\n",
    "        y_hat = self.forward(x)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(params=self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Run dm.prepare_data() and dm.setup() to get information from the dataset to build your model.\n",
    "multiclass_mlp = MulticlassTabularMLP(\n",
    "    input_size=len(dm.feature_cols),\n",
    "    output_size=dm.n_classes,\n",
    "    hidden_size=64, # 128\n",
    "    n_hidden_layers=3, # 3 - 5\n",
    "    dropout=0.2,\n",
    "    norm=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_mlp.named_parameters() # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_lightningmodel = MulticlassTabularLightningModule(\n",
    "    n_classes=dm.n_classes,\n",
    "    model=multiclass_mlp,\n",
    "    learning_rate=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    # optimizer=torch.optim.Adam(),\n",
    "    # loss_fn=loss_fn,\n",
    "    devices=\"auto\", # (os.cpu_count() / 2)\n",
    "    # metrics=metrics,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', min_delta=0.00, patience=5),\n",
    "        # L.ModelCheckpoint(\n",
    "        #     monitor=\"val_loss\",\n",
    "        #     mode=\"min\",\n",
    "        #     save_top_k=1,\n",
    "        #     save_path=f\"{save_path}/checkpoints\",\n",
    "        #     filename=\"best_model\",\n",
    "        # ),\n",
    "    ],\n",
    "    logger=CSVLogger(save_dir=\"logs/\", name=\"my-model\"),\n",
    "    # min_epochs=1,\n",
    "    max_epochs=100,\n",
    "    precision='bf16-mixed',\n",
    ")\n",
    "\n",
    "# # Create a Tuner\n",
    "tuner = Tuner(trainer)\n",
    "\n",
    "# finds learning rate automatically\n",
    "# sets hparams.lr or hparams.learning_rate to that learning rate\n",
    "lr_finder = tuner.lr_find(multiclass_lightningmodel, datamodule=dm)\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "#fig.savefig(\"lr_suggest.pdf\")\n",
    "# get suggestion\n",
    "new_lr = lr_finder.suggestion()\n",
    "print(new_lr)\n",
    "# update hparams of the model\n",
    "multiclass_lightningmodel.learning_rate = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: train.fit(), .validate(), .test() and .predict() all (re-)run dm.setup() when dataloader(s) are passed to these methods and pass the 'stage' parameter to .setup() to control the creation of the dataloaders.\n",
    "#       When a datamodule is passed to these methods, a hook that generates the respective dataloader(s) is called. When\n",
    "#       Validation is usually done during training, traditionally after each training epoch. It can be used for hyperparameter optimization or tracking model performance during training. It’s a part of the training process.\n",
    "#       Testing is usually done once we are satisfied with the training and only with the best model selected from the validation metrics.\n",
    "# trainer.fit(model=multiclass_lightningmodel, train_dataloaders=dm.train_dataloader()) # TODO: stage of The dataloader to use\n",
    "trainer.fit(model=multiclass_lightningmodel, train_dataloaders=dm.train_dataloader(), val_dataloaders=dm.val_dataloader()) # stage of the dataloader to use\n",
    "# trainer.validate() \n",
    "# trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\n",
    "\n",
    "aggreg_metrics = []\n",
    "agg_col = \"epoch\"\n",
    "for i, dfg in metrics.groupby(agg_col):\n",
    "    agg = dict(dfg.mean())\n",
    "    agg[agg_col] = i\n",
    "    aggreg_metrics.append(agg)\n",
    "\n",
    "df_metrics = pd.DataFrame(aggreg_metrics)\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 4))\n",
    "# Plot 1: Loss\n",
    "df_metrics[[\"train_loss\", \"val_loss\"]].plot(\n",
    "    ax=axes[0], grid=True, legend=True, xlabel=\"Epoch\", ylabel=\"Loss\"\n",
    ")\n",
    "# Plot 2: Accuracy\n",
    "df_metrics[[\"train_F1_macro_weighted\", \"val_F1_macro_weighted\"]].plot(\n",
    "    ax=axes[1], grid=True, legend=True, xlabel=\"Epoch\", ylabel=\"Accuracy\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = trainer.test(model=multiclass_lightningmodel, dataloaders=dm.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score[0]['test_F1_macro_weighted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data and evaluate\n",
    "preds_y_test = torch.cat(trainer.predict(model=multiclass_lightningmodel, dataloaders=dm.test_dataloader()))\n",
    "# inverse transform to get back to original labels\n",
    "preds_y_test = dm.label_encoder_target.inverse_transform(preds_y_test.reshape(-1, 1))\n",
    "y_test = dm.label_encoder_target.inverse_transform(dm.test_dataset.get_dataframe.iloc[:, -1].values.reshape(-1, 1))\n",
    "# calculate classification report\n",
    "print(classification_report(y_test, preds_y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN with HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performe HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptunaObjective(object):\n",
    "\n",
    "    def __init__(self, dm: TabularDataModuleClassificationPACKAGING = None):\n",
    "        self.dm = dm\n",
    "        self.dm.prepare_data()\n",
    "        self.dm.setup(stage='fit')\n",
    "\n",
    "    def __call__(self, trial: optuna.Trial) -> float:\n",
    "        \n",
    "        # joblib.dump(study, 'study.pkl')\n",
    "\n",
    "        hp_space_optuna = {\n",
    "            'hidden_size': trial.suggest_categorical('hidden_size', [8, 16, 32, 64, 128]), # number of neurons in each layer\n",
    "            'n_hidden_layers': trial.suggest_int(\"n_hidden_layers\", 1, 10), # number of layers\n",
    "            'batch_size': trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128]), # number of samples per batch\n",
    "            'dropout': trial.suggest_categorical(\"dropout\", [0.0, 0.1, 0.2, 0.3, 0.5]),\n",
    "        }\n",
    "\n",
    "        self.dm.batch_size = hp_space_optuna['batch_size']\n",
    "\n",
    "        multiclass_mlp = MulticlassTabularMLP(\n",
    "            input_size=len(self.dm.feature_cols),\n",
    "            output_size=self.dm.n_classes,\n",
    "            hidden_size=hp_space_optuna['hidden_size'],\n",
    "            n_hidden_layers=hp_space_optuna['n_hidden_layers'],\n",
    "            dropout=hp_space_optuna['dropout'],\n",
    "            norm=True,\n",
    "        )\n",
    "\n",
    "        multiclass_lightningmodel = MulticlassTabularLightningModule(\n",
    "            n_classes=self.dm.n_classes,\n",
    "            model=multiclass_mlp,\n",
    "            learning_rate=0.001,\n",
    "        )\n",
    "\n",
    "        trainer = L.Trainer(\n",
    "            devices=\"auto\", # (os.cpu_count() / 2)\n",
    "            callbacks=[\n",
    "                EarlyStopping(monitor='val_loss', min_delta=0.00, patience=5),\n",
    "            ],\n",
    "            logger=CSVLogger(save_dir=\"logs/\", name=\"my-model\"),\n",
    "            max_epochs=100,\n",
    "            precision='bf16-mixed',\n",
    "        )\n",
    "\n",
    "        # Create a Tuner\n",
    "        tuner = Tuner(trainer)\n",
    "        lr_finder = tuner.lr_find(multiclass_lightningmodel, datamodule=self.dm) # finds learning rate automatically\n",
    "        new_lr = lr_finder.suggestion()\n",
    "        multiclass_lightningmodel.learning_rate = new_lr # update hparams of the model\n",
    "\n",
    "        trainer.fit(\n",
    "            model=multiclass_lightningmodel,\n",
    "            train_dataloaders=self.dm.train_dataloader(),\n",
    "            val_dataloaders=self.dm.val_dataloader()\n",
    "        )\n",
    "\n",
    "        self.score = trainer.test(model=multiclass_lightningmodel, dataloaders=self.dm.test_dataloader())\n",
    "\n",
    "        return score[0]['test_F1_macro_weighted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper-parameter space, model + training, optimization metric via Objective\n",
    "objective = OptunaObjective(\n",
    "    dm=TabularDataModuleClassificationPACKAGING(\n",
    "        data_dir='../../data/output/df_ml.csv',\n",
    "        continuous_cols=['material_weight'],\n",
    "        categorical_cols=[\n",
    "            'material_number',\n",
    "            'brand',\n",
    "            'product_area',\n",
    "            'core_segment',\n",
    "            'component',\n",
    "            'manufactoring_location',\n",
    "            'characteristic_value',\n",
    "            'packaging_code'\n",
    "        ],\n",
    "        target=['packaging_category'],\n",
    "        oversampling=True,\n",
    "        test_size=0.2,\n",
    "        val_size=0.2,\n",
    "        batch_size=64,\n",
    "        SEED=SEED # Ensure same data split as in other notebooks\n",
    "    )\n",
    ")\n",
    "\n",
    "# define and run study for optimization\n",
    "# define search strategy: RandomSampler\n",
    "# study = optuna.create_study(direction=\"maximize\", sampler=optuna.RandomSampler(seed=SEED))\n",
    "# define search strategy: TPESampler = bayesian optimizer with a tree-structured Parzen Estimator\n",
    "study = optuna.create_study(\n",
    "    study_name=\"nn_optuna\",\n",
    "    # storage=optuna_storage,\n",
    "    # load_if_exists=True,\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=SEED)\n",
    ")\n",
    "\n",
    "# define duration of the optimization process by and/or number_of_trails and timeout\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=60,\n",
    "    # timeout=600, \n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print optimization results\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(\"  Performance: \", best_trial.value)\n",
    "print('  Best trial:', best_trial.params)\n",
    "# print(\"  Params: \")\n",
    "# for key, value in trial.params.items():\n",
    "#     print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse HPO\n",
    "Auswertung Optuna study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history of all trials\n",
    "hist = study.trials_dataframe()\n",
    "hist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot performance of all trials\n",
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the parameter relationship concerning performance\n",
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots the interactive visualization of the high-dimensional parameter relationship\n",
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots parameter interactive chart from we can choose which hyperparameter space has to explore\n",
    "optuna.visualization.plot_contour(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define best model\n",
    "\n",
    "best_params = best_trial.params\n",
    "# best_params = {\n",
    "#     'max_depth': 50,\n",
    "#     'criterion': 'entropy'\n",
    "# }\n",
    "\n",
    "# Test best model on test data again\n",
    "\n",
    "dm=TabularDataModuleClassificationPACKAGING(\n",
    "    data_dir='../../data/output/df_ml.csv',\n",
    "    continuous_cols=['material_weight'],\n",
    "    categorical_cols=[\n",
    "        'material_number',\n",
    "        'brand',\n",
    "        'product_area',\n",
    "        'core_segment',\n",
    "        'component',\n",
    "        'manufactoring_location',\n",
    "        'characteristic_value',\n",
    "        'packaging_code'\n",
    "    ],\n",
    "    target=['packaging_category'],\n",
    "    oversampling=True,\n",
    "    test_size=0.2,\n",
    "    val_size=0.2,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    SEED=SEED # Ensure same data split as in other notebooks\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup(stage='fit')\n",
    "\n",
    "# model\n",
    "best_model = MulticlassTabularMLP(\n",
    "    input_size=len(dm.feature_cols),\n",
    "    output_size=dm.n_classes,\n",
    "    hidden_size=best_params['hidden_size'],\n",
    "    n_hidden_layers=best_params['n_hidden_layers'],\n",
    "    dropout=best_params['dropout'],\n",
    "    norm=True,\n",
    ")\n",
    "\n",
    "multiclass_lightningmodel = MulticlassTabularLightningModule(\n",
    "    n_classes=dm.n_classes,\n",
    "    model=multiclass_mlp,\n",
    "    learning_rate=0.001,\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    devices=\"auto\", # (os.cpu_count() / 2)\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', min_delta=0.00, patience=5),\n",
    "    ],\n",
    "    logger=CSVLogger(save_dir=\"logs/\", name=\"my-model\"),\n",
    "    max_epochs=100,\n",
    "    precision='bf16-mixed',\n",
    ")\n",
    "\n",
    "tuner = Tuner(trainer)\n",
    "lr_finder = tuner.lr_find(multiclass_lightningmodel, datamodule=dm) # finds learning rate automatically\n",
    "new_lr = lr_finder.suggestion()\n",
    "multiclass_lightningmodel.learning_rate = new_lr # update hparams of the model\n",
    "\n",
    "trainer.fit(\n",
    "    model=multiclass_lightningmodel,\n",
    "    train_dataloaders=dm.train_dataloader(),\n",
    "    val_dataloaders=dm.val_dataloader()\n",
    ")\n",
    "\n",
    "score = trainer.test(model=multiclass_lightningmodel, dataloaders=dm.test_dataloader())\n",
    "print(f\"test_F1_macro_weighted: {score[0]['test_F1_macro_weighted']}\")\n",
    "\n",
    "# make predictions on test data and evaluate\n",
    "preds_y_test = torch.cat(trainer.predict(model=multiclass_lightningmodel, dataloaders=dm.test_dataloader()))\n",
    "# inverse transform to get back to original labels\n",
    "preds_y_test = dm.label_encoder_target.inverse_transform(preds_y_test.reshape(-1, 1))\n",
    "y_test = dm.label_encoder_target.inverse_transform(dm.test_dataset.get_dataframe.iloc[:, -1].values.reshape(-1, 1))\n",
    "# calculate classification report\n",
    "print(classification_report(y_test, preds_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_ml_packaging_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
